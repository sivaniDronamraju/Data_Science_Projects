{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdjGGySV3PMb"
      },
      "outputs": [],
      "source": [
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y56J2fBR6ESA",
        "outputId": "f500b67f-ab5f-4e8b-87dc-efe64d65cc67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-05 20:58:51--  https://snap.stanford.edu/data/finefoods.txt.gz\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 122104202 (116M) [application/x-gzip]\n",
            "Saving to: ‘finefoods.txt.gz’\n",
            "\n",
            "finefoods.txt.gz    100%[===================>] 116.45M  33.2MB/s    in 6.4s    \n",
            "\n",
            "2023-12-05 20:58:57 (18.3 MB/s) - ‘finefoods.txt.gz’ saved [122104202/122104202]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://snap.stanford.edu/data/finefoods.txt.gz\n",
        "!gzip -d finefoods.txt.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "18k4JqtX6x0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98e4f756-60f6-427a-ffd3-a36314793906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "product/productId: B001E4KFG0\n",
            "\n",
            "review/userId: A3SGXH7AUHU8GW\n",
            "\n",
            "review/profileName: delmartian\n",
            "\n",
            "review/helpfulness: 1/1\n",
            "\n",
            "review/score: 5.0\n",
            "\n",
            "review/time: 1303862400\n",
            "\n",
            "review/summary: Good Quality Dog Food\n",
            "\n",
            "review/text: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
            "\n",
            "\n",
            "\n",
            "product/productId: B00813GRG4\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open('finefoods.txt', 'r', encoding='latin-1') as file:\n",
        "    for _ in range(10):  # Print the first 10 lines\n",
        "        print(file.readline())\n",
        "# print only the first few lines of the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GEjjnwZ97-wM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc89024f-51c1-41b7-8d3e-2f574b9c2ed2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    productId          userId                      profileName helpfulness  \\\n",
            "0  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian         1/1   \n",
            "1  B00813GRG4  A1D87F6ZCVE5NK                           dll pa         0/0   \n",
            "2  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"         1/1   \n",
            "3  B000UA0QIQ  A395BORC6FGVXV                             Karl         3/3   \n",
            "4  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"         0/0   \n",
            "\n",
            "  score        time                summary  \\\n",
            "0   5.0  1303862400  Good Quality Dog Food   \n",
            "1   1.0  1346976000      Not as Advertised   \n",
            "2   4.0  1219017600  \"Delight\" says it all   \n",
            "3   2.0  1307923200         Cough Medicine   \n",
            "4   5.0  1350777600            Great taffy   \n",
            "\n",
            "                                                text  \n",
            "0  I have bought several of the Vitality canned d...  \n",
            "1  Product arrived labeled as Jumbo Salted Peanut...  \n",
            "2  This is a confection that has been around a fe...  \n",
            "3  If you are looking for the secret ingredient i...  \n",
            "4  Great taffy at a great price.  There was a wid...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the file line by line and extract relevant information\n",
        "data = {'productId': [], 'userId': [], 'profileName': [], 'helpfulness': [], 'score': [], 'time': [], 'summary': [], 'text': []}\n",
        "\n",
        "with open('finefoods.txt', 'r', encoding='latin-1') as file:\n",
        "    current_review = {}\n",
        "    for line in file:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            parts = line.split(': ', 1)\n",
        "            if len(parts) == 2:\n",
        "                key, value = parts\n",
        "                current_review[key] = value\n",
        "        else:\n",
        "            # End of review, add it to the data dictionary\n",
        "            data['productId'].append(current_review.get('product/productId', ''))\n",
        "            data['userId'].append(current_review.get('review/userId', ''))\n",
        "            data['profileName'].append(current_review.get('review/profileName', ''))\n",
        "            data['helpfulness'].append(current_review.get('review/helpfulness', ''))\n",
        "            data['score'].append(current_review.get('review/score', ''))\n",
        "            data['time'].append(current_review.get('review/time', ''))\n",
        "            data['summary'].append(current_review.get('review/summary', ''))\n",
        "            data['text'].append(current_review.get('review/text', ''))\n",
        "            current_review = {}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAdW6LLkUTsi"
      },
      "source": [
        " * UTF-8 encoding format if you have a large collection of foreign language resources because it allows you to access a greater number of foreign characters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "i0Wt2cRx8Wft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dcefe72-b197-4c79-add1-93257e65e874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7194149053135253\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.59      0.69      0.63     10326\n",
            "         2.0       0.43      0.29      0.35      5855\n",
            "         3.0       0.44      0.38      0.41      8485\n",
            "         4.0       0.41      0.41      0.41     16123\n",
            "         5.0       0.85      0.87      0.86     72902\n",
            "\n",
            "    accuracy                           0.72    113691\n",
            "   macro avg       0.54      0.53      0.53    113691\n",
            "weighted avg       0.71      0.72      0.72    113691\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Combine review/summary and review/text columns\n",
        "df['text'] = df['summary'] + ' ' + df['text']\n",
        "\n",
        "# Features and target variable\n",
        "X = df[['userId', 'profileName', 'time', 'text']]\n",
        "y = df['score']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize text data\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_text = vectorizer.fit_transform(X_train['text'])\n",
        "X_test_text = vectorizer.transform(X_test['text'])\n",
        "\n",
        "# Train Naive Bayes Classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_text, y_train)\n",
        "\n",
        "# predictions\n",
        "y_pred = nb_classifier.predict(X_test_text)\n",
        "\n",
        "# model evaluation\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsQcMNtPtf7y"
      },
      "source": [
        "Now cleaning the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hVj190xG-998",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "703ccc7f-3ced-4be9-cdad-174bfccf471e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "<ipython-input-10-1aae9b70e48a>:17: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, 'html.parser')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7342533709792332\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.63      0.66      0.65     10326\n",
            "         2.0       0.64      0.14      0.23      5855\n",
            "         3.0       0.52      0.26      0.34      8485\n",
            "         4.0       0.45      0.36      0.40     16123\n",
            "         5.0       0.80      0.93      0.86     72902\n",
            "\n",
            "    accuracy                           0.73    113691\n",
            "   macro avg       0.61      0.47      0.50    113691\n",
            "weighted avg       0.71      0.73      0.71    113691\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english')) | ENGLISH_STOP_WORDS\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    return soup.get_text()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()    # Convert to lowercase\n",
        "    text = remove_html_tags(text)    # Remove HTML tags\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)    # Remove special characters, numbers, and punctuation\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stop words and lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    # Join tokens back into text\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply text cleaning to 'text' column in the DataFrame\n",
        "df['text'] = df['text'].apply(clean_text)\n",
        "\n",
        "X = df[['userId', 'profileName', 'time', 'text']]\n",
        "y = df['score']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_text = vectorizer.fit_transform(X_train['text'])\n",
        "X_test_text = vectorizer.transform(X_test['text'])\n",
        "\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_text, y_train)\n",
        "\n",
        "y_pred = nb_classifier.predict(X_test_text)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is extracted from the The Amazon Fine Foods reviews dataset. In order to predict review scores, important features were extracted. 'userId' and 'profileName' were selected in order to comprehend score patterns linked to certain individuals or profiles. Examining temporal trends and determining if sentiment evolved over time was made possible by the 'time' feature, which represented review timestamps. The classifier received a comprehensive input from the 'text' feature, which combined'review/summary' and'review/text',combining summary and detailed text.\n"
      ],
      "metadata": {
        "id": "HKiOQvg_CAnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then i cleaned the data.In the beginning, HTML tags were removed in order to concentrate just on content. Lowercasing ensured that words were treated consistently, and removing punctuation, special characters, and digits made the data easier to understand. Text was divided into meaningful units by tokenization, and more significant terms were given priority when common stop words were eliminated. Lemmatization helped with uniform treatment by reducing words to their most basic form and dimensionality.\n",
        "\n",
        "There were notable gains when the cleaned text was used as the main input for the Naive Bayes Classifier. Model accuracy increased from 71.9% to 73.4%, indicating that the data cleaning procedure had a beneficial effect."
      ],
      "metadata": {
        "id": "4eguHWeOETmB"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}